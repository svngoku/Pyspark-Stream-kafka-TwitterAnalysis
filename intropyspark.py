# -*- coding: utf-8 -*-
"""IntroPySpark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cE_1dg8qnOKUFP8DPtV4uAzxMPNX5FL8
"""

!pip3 install pyspark

!sudo apt install python3.9

!alias python='/usr/bin/python3.9'

!python --version

"""# PySpark : Excercies RDD"""

from pyspark.sql import SparkSession

from pathlib import Path

spark = SparkSession.builder.appName("MyApp").getOrCreate()

rdd_context = spark.sparkContext.textFile("./datas/datas.txt")

rdd_context.collect()

"""## Map"""

rdd_file = rdd_context.map(lambda x: x.upper())

print(rdd_file.collect())

"""## FlatMap"""

flatmap = ["mot1 mot2 ", "mot3 mot4 "," mot5 mot6 "]

rdd_flatmap = spark.sparkContext.parallelize(flatmap)

for flatten in rdd_flatmap.collect():
  print(flatten)

rdd_flatmap2 = rdd_flatmap.flatMap(lambda x: x.split(" "))
for flatten2 in rdd_flatmap2.collect():
  print(flatten2)

def mapplify_mapflatify(datas):
  rdd_context = spark.sparkContext.parallelize(datas)
  rdd_mapped = rdd_context.map(lambda x: x)
  rdd_flatmapped = rdd_mapped.flatMap(lambda x: x)
  for element in rdd_flatmapped.collect():
    print(element)

mapplify_mapflatify(flatmap)

"""## Filter"""

filter_datas = ["Pierre", "Alpha", "Mehdi", "Jean-Pierre", "Thierry"]

def find_composed_name(datas):
  filter_rdd_context = spark.sparkContext.parallelize(datas)
  composed_name = filter_rdd_context.filter(lambda x: "-" in x).collect()
  return composed_name

find_composed_name(filter_datas)

"""## Distinct"""

listed_names = ["Pierre", "Alpha", "Mehdi", "Jean-Pierre", "Thierry", "Pierre"]

def show_distinct(datas):
  disctinct_rdd_context = spark.sparkContext.parallelize(datas)
  disctinted_datas = disctinct_rdd_context.distinct()
  
  return disctinted_datas.collect()

show_distinct(listed_names)

"""## GroupBy"""

def group_names(datas):
  groupnames_rdd_context = spark.sparkContext.parallelize(datas)
  # Regrouper sur chaque premier élément de chaque caractère de la liste 
  groupnames_datas = groupnames_rdd_context.groupBy(lambda x: x == "P") \
    .mapValues(list) \
    .collect()

  print(groupnames_datas)

group_names(listed_names)

"""## Sample"""

def sampling(datas):
  sampling_rdd_context = spark.sparkContext.parallelize(datas)
  sampling_datas = sampling_rdd_context.sample(0.5, 9).collect()
  
  return sampling_datas

sampling(listed_names)

"""# Programmation RDDs

## Union
"""

liste1 = [1, 2, 3]
liste2 = [3, 4, 5]

rdd1 = spark.sparkContext.parallelize(liste1)
rdd2 = spark.sparkContext.parallelize(liste2)

unionDF = rdd1.union(rdd2)

unionDF.collect()

"""## Substract"""

rdd1.subtract(rdd2).collect()

rdd2.subtract(rdd1).collect()

"""## Cartesien"""

rdd1.cartesian(rdd2).collect()

rdd2.cartesian(rdd1).collect()

"""## Reduce"""

listed_rdd = range(1, 6)

listed_ctx_rdd = spark.sparkContext.parallelize(listed_rdd)

listed_ctx_rdd.reduce(lambda x, y: x + y)

"""# Programmation RDDs Advanced

## RDD Pair : `Map()` and `KeyBy()`
"""

key_pair_ds = ["cle1 valeur1", "cle2 valeur2", "cle3 valeur3"]

def map_keyyed(datas):
  transform_datas = spark.sparkContext.parallelize(datas)
  rdd_flatmapped = transform_datas.map(
      lambda x:
      (x, 1)
  )
  rdd_keyby = rdd_flatmapped.keyBy(lambda x: (x, 1)).collect()
  print(rdd_keyby)

  #return rdd_keyby

map_keyyed(key_pair_ds)

"""## `GroupByKey`"""

to_regroup = ["0, 11", "1, 11", "0, 4", "2, 8", "1, 1", "9, 8"]

def regroup_datas(datas):
  ctx_transform_datas = spark.sparkContext.parallelize(datas)
  rdd_grouped = ctx_transform_datas.groupByKey().mapValues(list).collect()

  return rdd_grouped

# regroup_datas(to_regroup)

"""## reduceByKey"""











