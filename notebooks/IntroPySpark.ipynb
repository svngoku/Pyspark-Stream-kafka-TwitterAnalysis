{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "yfXffjIiMisP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfXffjIiMisP",
        "outputId": "03ec4deb-2a87-43f4-ac5d-4e545cff74aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Using cached pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "Using legacy 'setup.py install' for pyspark, since package 'wheel' is not installed.\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Running setup.py install for pyspark ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed py4j-0.10.9.5 pyspark-3.3.0\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
            "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.10/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip3 install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "A3uI622DwGAI",
      "metadata": {
        "id": "A3uI622DwGAI"
      },
      "outputs": [],
      "source": [
        "!alias python=python3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "vy6OH6U7ptGH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy6OH6U7ptGH",
        "outputId": "ee183e8c-75cb-43af-dd6d-10588676f3ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.9.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "em5c-K_1VJug",
      "metadata": {
        "id": "em5c-K_1VJug"
      },
      "source": [
        "# PySpark : Excercies RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "db86ffbd-5728-4101-9a1e-5332c0cd6f18",
      "metadata": {
        "id": "db86ffbd-5728-4101-9a1e-5332c0cd6f18"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0431058a",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "236e8b5e-8a27-4998-992e-20eec958accf",
      "metadata": {
        "id": "236e8b5e-8a27-4998-992e-20eec958accf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "22/06/22 00:29:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "ename": "Py4JError",
          "evalue": "An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\n\tat py4j.Gateway.invoke(Gateway.java:237)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/svngoku/Documents/global_projects/DataScience/HadoopMapReduce/Pyspark/IntroPySpark.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/svngoku/Documents/global_projects/DataScience/HadoopMapReduce/Pyspark/IntroPySpark.ipynb#ch0000007?line=0'>1</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mMyApp\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mgetOrCreate()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/sql/session.py:272\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    270\u001b[0m     \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m     session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_options)\n\u001b[1;32m    273\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[39mgetattr\u001b[39m(\n\u001b[1;32m    275\u001b[0m         \u001b[39mgetattr\u001b[39m(session\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\u001b[39m.\u001b[39mapplyModifiableSettings(session\u001b[39m.\u001b[39m_jsparkSession, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/sql/session.py:307\u001b[0m, in \u001b[0;36mSparkSession.__init__\u001b[0;34m(self, sparkContext, jsparkSession, options)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[39mgetattr\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mapplyModifiableSettings(\n\u001b[1;32m    304\u001b[0m             jsparkSession, options\n\u001b[1;32m    305\u001b[0m         )\n\u001b[1;32m    306\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m         jsparkSession \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mSparkSession(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc(), options)\n\u001b[1;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[39mgetattr\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mapplyModifiableSettings(\n\u001b[1;32m    310\u001b[0m         jsparkSession, options\n\u001b[1;32m    311\u001b[0m     )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1579\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1580\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1581\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1584\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1585\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1586\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[1;32m   1588\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1589\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n",
            "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\n\tat py4j.Gateway.invoke(Gateway.java:237)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26805cfe",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da553fe1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c349051",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cE0c1-NEO8dm",
      "metadata": {
        "id": "cE0c1-NEO8dm"
      },
      "outputs": [],
      "source": [
        "rdd_context = spark.sparkContext.textFile(\"./datas/datas.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c0ea31b0-696d-4721-ac0d-d429ff3cb16a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0ea31b0-696d-4721-ac0d-d429ff3cb16a",
        "outputId": "f13c5c6b-9302-4396-f961-018b90714502"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['mon joli fichier texte du futur']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd_context.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8a28977-250a-422d-98c2-d49dbbe3c6f5",
      "metadata": {
        "id": "d8a28977-250a-422d-98c2-d49dbbe3c6f5"
      },
      "source": [
        "## Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "jrmBmfADPTCv",
      "metadata": {
        "id": "jrmBmfADPTCv"
      },
      "outputs": [],
      "source": [
        "rdd_file = rdd_context.map(lambda x: x.upper())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d1e777d6-4eea-4879-a132-744b58d7b519",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1e777d6-4eea-4879-a132-744b58d7b519",
        "outputId": "c9d18372-c0f0-4df0-ab61-a540d0bd2eb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['MON JOLI FICHIER TEXTE DU FUTUR']\n"
          ]
        }
      ],
      "source": [
        "print(rdd_file.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2uT93rjCPiPG",
      "metadata": {
        "id": "2uT93rjCPiPG"
      },
      "source": [
        "## FlatMap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cerBZ-DXPfOS",
      "metadata": {
        "id": "cerBZ-DXPfOS"
      },
      "outputs": [],
      "source": [
        "flatmap = [\"mot1 mot2 \", \"mot3 mot4 \",\" mot5 mot6 \"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ffba55f3-56dc-41c4-a169-b81e13c849c0",
      "metadata": {
        "id": "ffba55f3-56dc-41c4-a169-b81e13c849c0"
      },
      "outputs": [],
      "source": [
        "rdd_flatmap = spark.sparkContext.parallelize(flatmap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "962db98d-d1af-4266-833f-602bd9fefa4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "962db98d-d1af-4266-833f-602bd9fefa4b",
        "outputId": "a9ca0079-0c4c-43be-acdb-2d79739c5f47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mot1 mot2 \n",
            "mot3 mot4 \n",
            " mot5 mot6 \n"
          ]
        }
      ],
      "source": [
        "for flatten in rdd_flatmap.collect():\n",
        "  print(flatten)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "EviKbvmY9tkZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EviKbvmY9tkZ",
        "outputId": "f977b358-c983-4217-d690-a53e37866e9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mot1\n",
            "mot2\n",
            "\n",
            "mot3\n",
            "mot4\n",
            "\n",
            "\n",
            "mot5\n",
            "mot6\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rdd_flatmap2 = rdd_flatmap.flatMap(lambda x: x.split(\" \"))\n",
        "for flatten2 in rdd_flatmap2.collect():\n",
        "  print(flatten2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "CNjTn3ys9th4",
      "metadata": {
        "id": "CNjTn3ys9th4"
      },
      "outputs": [],
      "source": [
        "def mapplify_mapflatify(datas):\n",
        "  rdd_context = spark.sparkContext.parallelize(datas)\n",
        "  rdd_mapped = rdd_context.map(lambda x: x)\n",
        "  rdd_flatmapped = rdd_mapped.flatMap(lambda x: x)\n",
        "  for element in rdd_flatmapped.collect():\n",
        "    print(element)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "iQXmaBK2_BV0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQXmaBK2_BV0",
        "outputId": "bd4e92b7-7216-4310-9361-101a37a861f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "m\n",
            "o\n",
            "t\n",
            "1\n",
            " \n",
            "m\n",
            "o\n",
            "t\n",
            "2\n",
            " \n",
            "m\n",
            "o\n",
            "t\n",
            "3\n",
            " \n",
            "m\n",
            "o\n",
            "t\n",
            "4\n",
            " \n",
            " \n",
            "m\n",
            "o\n",
            "t\n",
            "5\n",
            " \n",
            "m\n",
            "o\n",
            "t\n",
            "6\n",
            " \n"
          ]
        }
      ],
      "source": [
        "mapplify_mapflatify(flatmap)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TUcfJuc_AS0U",
      "metadata": {
        "id": "TUcfJuc_AS0U"
      },
      "source": [
        "## Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "r-Z-OPrTAWvO",
      "metadata": {
        "id": "r-Z-OPrTAWvO"
      },
      "outputs": [],
      "source": [
        "filter_datas = [\"Pierre\", \"Alpha\", \"Mehdi\", \"Jean-Pierre\", \"Thierry\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "-RFXaFk4GId8",
      "metadata": {
        "id": "-RFXaFk4GId8"
      },
      "outputs": [],
      "source": [
        "def find_composed_name(datas):\n",
        "  filter_rdd_context = spark.sparkContext.parallelize(datas)\n",
        "  composed_name = filter_rdd_context.filter(lambda x: \"-\" in x).collect()\n",
        "  return composed_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "hM_atvo8AW3F",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM_atvo8AW3F",
        "outputId": "6ed8a0f5-c03a-4553-ba93-04f41d2b62c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Jean-Pierre']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_composed_name(filter_datas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ucrLObvtH5y-",
      "metadata": {
        "id": "ucrLObvtH5y-"
      },
      "source": [
        "## Distinct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "93CQ3CDGAW6F",
      "metadata": {
        "id": "93CQ3CDGAW6F"
      },
      "outputs": [],
      "source": [
        "listed_names = [\"Pierre\", \"Alpha\", \"Mehdi\", \"Jean-Pierre\", \"Thierry\", \"Pierre\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5kni0vxXAW8k",
      "metadata": {
        "id": "5kni0vxXAW8k"
      },
      "outputs": [],
      "source": [
        "def show_distinct(datas):\n",
        "  disctinct_rdd_context = spark.sparkContext.parallelize(datas)\n",
        "  disctinted_datas = disctinct_rdd_context.distinct()\n",
        "  \n",
        "  return disctinted_datas.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "_JKI1W5jAW_a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JKI1W5jAW_a",
        "outputId": "d07fa329-f88b-4464-9747-16e2a7d34b00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Pierre', 'Alpha', 'Mehdi', 'Jean-Pierre', 'Thierry']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "show_distinct(listed_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ptRyaDIGRVRZ",
      "metadata": {
        "id": "ptRyaDIGRVRZ"
      },
      "source": [
        "## GroupBy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "Q3qDu3BiAXCi",
      "metadata": {
        "id": "Q3qDu3BiAXCi"
      },
      "outputs": [],
      "source": [
        "def group_names(datas):\n",
        "  groupnames_rdd_context = spark.sparkContext.parallelize(datas)\n",
        "  # Regrouper sur chaque premier élément de chaque caractère de la liste \n",
        "  groupnames_datas = groupnames_rdd_context.groupBy(lambda x: x == \"P\") \\\n",
        "    .mapValues(list) \\\n",
        "    .collect()\n",
        "\n",
        "  print(groupnames_datas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "4iCwkkzxRVJ2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iCwkkzxRVJ2",
        "outputId": "a06f1c94-4ad1-458d-e357-8c83380c7c0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(False, ['Pierre', 'Alpha', 'Mehdi', 'Jean-Pierre', 'Thierry', 'Pierre'])]\n"
          ]
        }
      ],
      "source": [
        "group_names(listed_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fsFJET0BXJIc",
      "metadata": {
        "id": "fsFJET0BXJIc"
      },
      "source": [
        "## Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "css4HybRXMFI",
      "metadata": {
        "id": "css4HybRXMFI"
      },
      "outputs": [],
      "source": [
        "def sampling(datas):\n",
        "  sampling_rdd_context = spark.sparkContext.parallelize(datas)\n",
        "  sampling_datas = sampling_rdd_context.sample(0.5, 9).collect()\n",
        "  \n",
        "  return sampling_datas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "Ajpgf4BdX-b_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajpgf4BdX-b_",
        "outputId": "3a621d0a-e215-4237-aba4-e2a0a954196c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Pierre',\n",
              " 'Pierre',\n",
              " 'Pierre',\n",
              " 'Pierre',\n",
              " 'Pierre',\n",
              " 'Pierre',\n",
              " 'Pierre',\n",
              " 'Pierre',\n",
              " 'Pierre',\n",
              " 'Alpha',\n",
              " 'Alpha',\n",
              " 'Alpha',\n",
              " 'Alpha',\n",
              " 'Alpha',\n",
              " 'Alpha',\n",
              " 'Alpha',\n",
              " 'Alpha',\n",
              " 'Alpha',\n",
              " 'Alpha',\n",
              " 'Alpha',\n",
              " 'Mehdi',\n",
              " 'Mehdi',\n",
              " 'Mehdi',\n",
              " 'Mehdi',\n",
              " 'Mehdi',\n",
              " 'Mehdi',\n",
              " 'Mehdi',\n",
              " 'Mehdi',\n",
              " 'Jean-Pierre',\n",
              " 'Jean-Pierre',\n",
              " 'Jean-Pierre',\n",
              " 'Jean-Pierre',\n",
              " 'Jean-Pierre',\n",
              " 'Jean-Pierre',\n",
              " 'Jean-Pierre',\n",
              " 'Jean-Pierre',\n",
              " 'Thierry',\n",
              " 'Thierry',\n",
              " 'Thierry',\n",
              " 'Thierry',\n",
              " 'Thierry',\n",
              " 'Pierre',\n",
              " 'Pierre',\n",
              " 'Pierre',\n",
              " 'Pierre',\n",
              " 'Pierre',\n",
              " 'Pierre',\n",
              " 'Pierre',\n",
              " 'Pierre',\n",
              " 'Pierre']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampling(listed_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p3ZJcX8pVCnv",
      "metadata": {
        "id": "p3ZJcX8pVCnv"
      },
      "source": [
        "# Programmation RDDs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_XxMtVMJbPd1",
      "metadata": {
        "id": "_XxMtVMJbPd1"
      },
      "source": [
        "## Union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "D5zg_FTAAXFi",
      "metadata": {
        "id": "D5zg_FTAAXFi"
      },
      "outputs": [],
      "source": [
        "liste1 = [1, 2, 3]\n",
        "liste2 = [3, 4, 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "EkdG6TcDAXIJ",
      "metadata": {
        "id": "EkdG6TcDAXIJ"
      },
      "outputs": [],
      "source": [
        "rdd1 = spark.sparkContext.parallelize(liste1)\n",
        "rdd2 = spark.sparkContext.parallelize(liste2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "TSzJ5_HlAXLJ",
      "metadata": {
        "id": "TSzJ5_HlAXLJ"
      },
      "outputs": [],
      "source": [
        "unionDF = rdd1.union(rdd2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "KeMge0ShAXOI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeMge0ShAXOI",
        "outputId": "a0ac59bf-1840-44a4-8c98-83ee815ebf70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 2, 3, 3, 4, 5]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unionDF.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aBfMfkgibSjE",
      "metadata": {
        "id": "aBfMfkgibSjE"
      },
      "source": [
        "## Substract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "_b5IdRbabWuf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b5IdRbabWuf",
        "outputId": "473f194e-10ab-4a0f-9fc8-61261777ec33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 2]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd1.subtract(rdd2).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "c4f0evyJAXRq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4f0evyJAXRq",
        "outputId": "0e4dd38e-9dd6-45cf-d2eb-4975da01edc8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[4, 5]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd2.subtract(rdd1).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uX_AEC6oc_7G",
      "metadata": {
        "id": "uX_AEC6oc_7G"
      },
      "source": [
        "## Cartesien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "RmynnX_XAXUh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmynnX_XAXUh",
        "outputId": "e74d7df5-fbaa-40f6-8a62-cb91010e8021"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(1, 3), (1, 4), (1, 5), (2, 3), (3, 3), (2, 4), (2, 5), (3, 4), (3, 5)]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd1.cartesian(rdd2).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "KIlW3NfAAXXg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIlW3NfAAXXg",
        "outputId": "2fc6644a-290b-4514-8d11-f31defde9869"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(3, 1), (3, 2), (3, 3), (4, 1), (5, 1), (4, 2), (4, 3), (5, 2), (5, 3)]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd2.cartesian(rdd1).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IaiccrAbd0ac",
      "metadata": {
        "id": "IaiccrAbd0ac"
      },
      "source": [
        "## Reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "uO2WzlepeGzF",
      "metadata": {
        "id": "uO2WzlepeGzF"
      },
      "outputs": [],
      "source": [
        "listed_rdd = range(1, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "dABs-IW9fX_Z",
      "metadata": {
        "id": "dABs-IW9fX_Z"
      },
      "outputs": [],
      "source": [
        "listed_ctx_rdd = spark.sparkContext.parallelize(listed_rdd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "-zQ4oPTCAXag",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zQ4oPTCAXag",
        "outputId": "a4570b46-8adb-4759-b877-298a01185566"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "listed_ctx_rdd.reduce(lambda x, y: x + y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58GCIRhoihtJ",
      "metadata": {
        "id": "58GCIRhoihtJ"
      },
      "source": [
        "# Programmation RDDs Advanced "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02rzGBp7inK_",
      "metadata": {
        "id": "02rzGBp7inK_"
      },
      "source": [
        "## RDD Pair : `Map()` and `KeyBy()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "GKNGfAkDAXdg",
      "metadata": {
        "id": "GKNGfAkDAXdg"
      },
      "outputs": [],
      "source": [
        "key_pair_ds = [\"cle1 valeur1\", \"cle2 valeur2\", \"cle3 valeur3\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "NbC8WIBYAXgg",
      "metadata": {
        "id": "NbC8WIBYAXgg"
      },
      "outputs": [],
      "source": [
        "def map_keyyed(datas):\n",
        "  transform_datas = spark.sparkContext.parallelize(datas)\n",
        "  rdd_flatmapped = transform_datas.map(\n",
        "      lambda x:\n",
        "      (x, 1)\n",
        "  )\n",
        "  rdd_keyby = rdd_flatmapped.keyBy(lambda x: (x, 1)).collect()\n",
        "  print(rdd_keyby)\n",
        "\n",
        "  #return rdd_keyby"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "x8BqWKPfAXky",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8BqWKPfAXky",
        "outputId": "94782c4d-7491-4bc0-dfb6-d52cd52eea5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[((('cle1 valeur1', 1), 1), ('cle1 valeur1', 1)), ((('cle2 valeur2', 1), 1), ('cle2 valeur2', 1)), ((('cle3 valeur3', 1), 1), ('cle3 valeur3', 1))]\n"
          ]
        }
      ],
      "source": [
        "map_keyyed(key_pair_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4-Dz77spmObr",
      "metadata": {
        "id": "4-Dz77spmObr"
      },
      "source": [
        "## `GroupByKey`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "DYfpTPzrAXoD",
      "metadata": {
        "id": "DYfpTPzrAXoD"
      },
      "outputs": [],
      "source": [
        "to_regroup = [\"0, 11\", \"1, 11\", \"0, 4\", \"2, 8\", \"1, 1\", \"9, 8\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "Ou-mXBCwAXrT",
      "metadata": {
        "id": "Ou-mXBCwAXrT"
      },
      "outputs": [],
      "source": [
        "def regroup_datas(datas):\n",
        "  ctx_transform_datas = spark.sparkContext.parallelize(datas)\n",
        "  rdd_grouped = ctx_transform_datas.groupByKey().mapValues(list).collect()\n",
        "\n",
        "  return rdd_grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "-nulf1QeAXuC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-nulf1QeAXuC",
        "outputId": "c2fa0a40-811d-478a-e7a1-741e39a63e52"
      },
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-f1825f9b0814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mregroup_datas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_regroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-b8f57bc71d65>\u001b[0m in \u001b[0;36mregroup_datas\u001b[0;34m(datas)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregroup_datas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mctx_transform_datas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mrdd_grouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx_transform_datas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mrdd_grouped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 62) (bd48492faa8c executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2665, in combine\n    merger.mergeValues(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 253, in mergeValues\n    for k, v in iterator:\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor45.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 2665, in combine\n    merger.mergeValues(iterator)\n  File \"/usr/local/lib/python3.7/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 253, in mergeValues\n    for k, v in iterator:\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
          ]
        }
      ],
      "source": [
        "# regroup_datas(to_regroup)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9NCLYv78opZy",
      "metadata": {
        "id": "9NCLYv78opZy"
      },
      "source": [
        "## reduceByKey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RJy1g8uIi-ZH",
      "metadata": {
        "id": "RJy1g8uIi-ZH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "juRX7Jgxi-bb",
      "metadata": {
        "id": "juRX7Jgxi-bb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0GyHXNnUi-eC",
      "metadata": {
        "id": "0GyHXNnUi-eC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rgFeaaFNi-ge",
      "metadata": {
        "id": "rgFeaaFNi-ge"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3SmRYFLi-jU",
      "metadata": {
        "id": "d3SmRYFLi-jU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4nVN0nP1i-lj",
      "metadata": {
        "id": "4nVN0nP1i-lj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "TUcfJuc_AS0U",
        "p3ZJcX8pVCnv",
        "_XxMtVMJbPd1",
        "aBfMfkgibSjE",
        "uX_AEC6oc_7G",
        "IaiccrAbd0ac"
      ],
      "name": "IntroPySpark.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
