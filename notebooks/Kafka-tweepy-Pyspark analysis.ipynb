{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf34ae1",
   "metadata": {},
   "source": [
    "# Ingesting realtime tweets using Apache Kafka, Tweepy and Python\n",
    "\n",
    "## Purpose:\n",
    "\n",
    "Main data source for the lambda architecture pipeline\n",
    "uses twitter streaming API to simulate new events coming in every minute\n",
    "Kafka Producer sends the tweets as records to the Kafka Broker\n",
    "Contents:\n",
    "\n",
    "- Twitter setup\n",
    "- Defining the Kafka producer\n",
    "- Producing and sending records to the Kafka Broker\n",
    "- Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeef446b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "Collecting tweepy\n",
      "  Using cached tweepy-4.10.0-py3-none-any.whl (94 kB)\n",
      "Collecting pymongo\n",
      "  Downloading pymongo-4.1.1-cp37-cp37m-macosx_10_6_intel.whl (394 kB)\n",
      "\u001b[K     |████████████████████████████████| 394 kB 89 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kafka-python\n",
      "  Using cached kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "Collecting py4j==0.10.9.5\n",
      "  Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /Users/svngoku/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from tweepy) (2.27.1)\n",
      "Collecting requests-oauthlib<2,>=1.2.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib<4,>=3.2.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0\n",
      "  Using cached dnspython-2.2.1-py3-none-any.whl (269 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/svngoku/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/svngoku/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/svngoku/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/svngoku/opt/anaconda3/envs/tf/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=a587d64edeb6929b78afd9c78f1294dbf94997dc618e214d5b2d78ab8270f539\n",
      "  Stored in directory: /Users/svngoku/Library/Caches/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
      "Successfully built pyspark\n",
      "Installing collected packages: oauthlib, requests-oauthlib, pymongo, py4j, dnspython, tweepy, pyspark, kafka-python\n",
      "Successfully installed dnspython-2.2.1 kafka-python-2.0.2 oauthlib-3.2.0 py4j-0.10.9.5 pymongo-4.1.1 pyspark-3.3.0 requests-oauthlib-1.3.1 tweepy-4.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark tweepy pymongo kafka-python \"pymongo[srv]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a86a040b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.13\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af29a5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.0\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 1.8.0_292\n",
      "Branch HEAD\n",
      "Compiled by user ubuntu on 2022-06-09T19:58:58Z\n",
      "Revision f74867bddfbcdd4d08076db36851e88b15e66556\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d7162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.streaming import StreamingContext\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05592d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = \"tweet_stream\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2084e8f",
   "metadata": {},
   "source": [
    "## Base de données SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "028482e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are connected!\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\n",
    "    \"mongodb+srv://twitterbot:SaPDBU5WU3O349mn@cluster0.z5t7h.mongodb.net/?tls=true&tlsAllowInvalidCertificates=true\"\n",
    ")\n",
    "\n",
    "twitteranalysis_database = client.twitteranalysis\n",
    "\n",
    "try: \n",
    "    twitteranalysis_database.command(\"serverStatus\")\n",
    "except Exception as e: \n",
    "    print(e)\n",
    "else: \n",
    "    print(\"You are connected!\")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2216af7",
   "metadata": {},
   "source": [
    "#  Configuration de Twitter \n",
    "\n",
    "getting the API object using authorization information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d210853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter setup\n",
    "consumer_key = \"886bWUB38AHD1VC8vE777rVKs\"\n",
    "consumer_secret = \"QLTWRcxbmxjOAAJatf4WCbL7j5vQYiyhSImv00wLarPVctcXE4\"\n",
    "access_token = \"765095367067262976-Nz1XFSRSQjdd2MKYdPLiyKpjTUSsEoo\"\n",
    "access_token_secret = \"8GZJcQVFJEk4SBrGgjk0V1HXiFau920Jt62Dntgf65qug\"\n",
    "# Creating the authentication object\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "# Setting your access token and secret\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "# Creating the API object by passing in auth information\n",
    "api = tweepy.API(auth) \n",
    "StreamTweepy = tweepy.Stream(\n",
    "    consumer_key, \n",
    "    consumer_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c070c007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication Successful\n"
     ]
    }
   ],
   "source": [
    "#try connection\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication Successful\")\n",
    "except:\n",
    "    print(\"Authentication Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e1ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_timestamp(time):\n",
    "    mytime = datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\")\n",
    "    #mytime += timedelta(hours=1)   # the tweets are timestamped in GMT timezone, while I am in +1 timezone\n",
    "    return (mytime.strftime(\"%Y-%m-%d %H:%M:%S\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21457f",
   "metadata": {},
   "source": [
    "# Création du producer Kafka \n",
    "\n",
    "- specify the Kafka Broker\n",
    "- specify the topic name\n",
    "- optional: specify partitioning strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed455492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kafka_stream_tweets(data):\n",
    "    producer = KafkaProducer(\n",
    "        value_serializer=lambda m: json.dumps(m).encode('utf-8'),\n",
    "        bootstrap_servers='localhost:9092'\n",
    "    )\n",
    "    producer.send(topic_name, value=data)\n",
    "    date = datetime.now()\n",
    "    print(date,\"New Tweet ! (Len:\",len(data),\")\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e488e9",
   "metadata": {},
   "source": [
    "## Initialisation de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79865c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .appName(\"TwitterAnalysis\") \\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-streaming-kafka-0-10_2.11:2.2.0') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42a1563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization Spark\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", topic_name) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2c839",
   "metadata": {},
   "source": [
    "## Produire et envoyer des enregistrements au courtier Kafka\n",
    "\n",
    "1. Interroger l'objet de l'API Twitter\n",
    "2. Extraire les informations pertinentes dans la réponse\n",
    "3. Formater et envoyer les données appropriés sur le courtier Kafka.\n",
    "\n",
    "## Les tweets résultants ont les attributs suivants :\n",
    "- id\n",
    "- created_at\n",
    "- followers_count\n",
    "- localisation\n",
    "- text\n",
    "- nombre de favoris\n",
    "- nombre de retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f89fd0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_twitter_data():\n",
    "    tweets_rdd = []\n",
    "    res = api.search_tweets(q=\"harcelements\",count = 700)\n",
    "    for i in res:\n",
    "        record = ''\n",
    "        record += str(i.user.id_str)\n",
    "        record += ';'\n",
    "        record += str(i.created_at)\n",
    "        record += ';'\n",
    "        record += str(i.text)\n",
    "        record += ';'\n",
    "        record += str(i.user.description)\n",
    "        record += ';' \n",
    "        record += str(i.user.followers_count)\n",
    "        record += ';'\n",
    "        record += str(i.user.location)\n",
    "        record += ';'\n",
    "        record += str(i.favorite_count)\n",
    "        record += ';'\n",
    "        record += str(i.retweet_count)\n",
    "        record += ';'\n",
    "        tweets_rdd.append(record)\n",
    "    \n",
    "    return tweets_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66c89f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def make_my_tweet_cleaner(tweet):\n",
    "    tweet = sc.parallelize([tweet])\n",
    "    parsed = tweet\n",
    "    words = parsed.flatMap(lambda line: line.replace('\"',' ').replace(\"'\",' ').replace(\"(\",' ').replace(\")\",' ').replace(\"\\\\\",' ').replace(\".\",' ').split())\n",
    "    hashtags = words.filter(lambda w: re.findall(r'\\B#\\w*[a-zA-Z]+\\w*',w)).map(lambda x:(x, 1))\n",
    "    # clean the text by removing all emoji and mentionned peoples\n",
    "    print(words.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33db64ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTweets():\n",
    "    rdd_tweets = []\n",
    "    # Get tweets\n",
    "    try:\n",
    "        # Get tweets from Paris (geocode) and with the keywords \"harcelements\" \n",
    "        tweets = api.search_tweets(q=[\"harcelements\"], geocode = \"48.864716,2.349014,10km\", count=200)\n",
    "        for i in tweets:\n",
    "            record = ''\n",
    "            record += str(i.user.id_str)\n",
    "            #record += ';'\n",
    "            record += str(i.created_at)\n",
    "            #record += ';'\n",
    "            record += str(i.text)\n",
    "            rdd_tweets.append(record)\n",
    "        \n",
    "        return rdd_tweets\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error : \" + str(e))\n",
    "\n",
    "def sendTweets():\n",
    "    # Get tweets and send to Kafka\n",
    "    try:\n",
    "        # Send tweets\n",
    "        for tweet in get_twitter_data():\n",
    "            make_my_tweet_cleaner(tweet)\n",
    "            #kafka_stream_tweets(tweet)\n",
    "            #stream_tweets = twitteranalysis_database.stream_tweets\n",
    "            #stream_tweets.insert_many([{\"tweet\": tweet}])\n",
    "    except Exception as e:\n",
    "        print(\"Error : \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "776c8eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error : An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 1.0 failed 1 times, most recent failure: Lost task 6.0 in stage 1.0 (TID 14) (192.168.1.182 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 601, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: module 'pyspark.rdd' has no attribute 'T'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 601, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: module 'pyspark.rdd' has no attribute 'T'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/23 21:11:00 ERROR Executor: Exception in task 6.0 in stage 1.0 (TID 14)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 601, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: module 'pyspark.rdd' has no attribute 'T'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/06/23 21:11:00 ERROR Executor: Exception in task 1.0 in stage 1.0 (TID 9)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 601, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: module 'pyspark.rdd' has no attribute 'T'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/06/23 21:11:00 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 8)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 601, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: module 'pyspark.rdd' has no attribute 'T'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/06/23 21:11:00 ERROR Executor: Exception in task 7.0 in stage 1.0 (TID 15)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 601, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: module 'pyspark.rdd' has no attribute 'T'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/06/23 21:11:00 ERROR Executor: Exception in task 5.0 in stage 1.0 (TID 13)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 601, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: module 'pyspark.rdd' has no attribute 'T'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/06/23 21:11:00 ERROR Executor: Exception in task 3.0 in stage 1.0 (TID 11)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 601, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: module 'pyspark.rdd' has no attribute 'T'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/06/23 21:11:00 ERROR Executor: Exception in task 2.0 in stage 1.0 (TID 10)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 601, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: module 'pyspark.rdd' has no attribute 'T'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/06/23 21:11:00 ERROR Executor: Exception in task 4.0 in stage 1.0 (TID 12)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 601, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: module 'pyspark.rdd' has no attribute 'T'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/06/23 21:11:00 WARN TaskSetManager: Lost task 6.0 in stage 1.0 (TID 14) (192.168.1.182 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 601, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 71, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/homebrew/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "AttributeError: module 'pyspark.rdd' has no attribute 'T'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:555)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:713)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:695)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:508)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/06/23 21:11:00 ERROR TaskSetManager: Task 6 in stage 1.0 failed 1 times; aborting job\n"
     ]
    }
   ],
   "source": [
    "sendTweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593f17b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code de vectorisation des textes des tweets #\n",
    "# from pyspark.ml.feature import HashingTF, IDF \n",
    "# # Définition du traitement de HashingTF \n",
    "# nbfeatures=6000\n",
    "# hashingTF = HashingTF(inputCol=\"tokens\",outputCol=\"raw_features\",numFeatures=nbfeatures)\n",
    "# features_df = hashingTF.transform(data)\n",
    "# idf = IDF(inputCol=\"raw_features\", outputCol=\"features\") \n",
    "# idf_model = idf.fit(features_df)\n",
    "# scaled_features_df = idf_model.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14edbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_work(interval):\n",
    "    while True:\n",
    "        get_twitter_data()\n",
    "        #interval should be an integer, the number of seconds to wait\n",
    "        time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ec769",
   "metadata": {},
   "outputs": [],
   "source": [
    "periodic_work(60 * 0.1)  # get data every couple of minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f52fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a770f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_my_tweet_cleaner(tweet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
